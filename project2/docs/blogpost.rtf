{\rtf1\ansi\ansicpg1252\cocoartf2865
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Roman;\f1\froman\fcharset0 Times-Italic;\f2\fmodern\fcharset0 Courier;
}
{\colortbl;\red255\green255\blue255;\red109\green109\blue109;\red0\green0\blue0;\red0\green0\blue233;
}
{\*\expandedcolortbl;;\cssrgb\c50196\c50196\c50196;\cssrgb\c0\c0\c0;\cssrgb\c0\c0\c93333;
}
\margl1440\margr1440\vieww16520\viewh10260\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs24 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \
\pard\pardeftab720\sa240\partightenfactor0
\cf0 \strokec3 In this project, I aimed to answer a pertinent subquestion related to my overall thesis project. Overall, I seek to reconstruct the history of Mahmadou Lamine Dram\'e9, a nineteenth-century Islamic leader from what is now eastern Senegal. My specific research question was, simply put: 
\f1\i \strokec3 How did French officers talk about Mahmadou Lamine Dram\'e9 and his nascent attempt at state-building, and how can we compare this written narrative to the one that appears across the letters Mahmadou sent to French colonial administrators?
\f0\i0 \strokec3  This query matters to my work because it allows me to outline an essential historical misinterpretation of Mahmadou\'92s motives 
\f1\i \strokec3 vis-\'e0-vis
\f0\i0 \strokec3  the French colonial state\'97one that is described in the texts and has had wide ramifications across later historiography. This is a type of question which, I felt, was best approached by paying close attention to semantics and word use in texts.\
As a student in a digital humanities course at the Aga Khan University, I was expected to use digital methods to explore a research question. A plethora of machine-learning tools have been developed that are capable of digitally performing semantic analysis at massive scales. These were used experimentally in the series of processes, or pipeline, at the center of my project. My primary goal was to develop a usable text-mining methodology in order to access meanings and trends across a series of texts. Topic modelling and clustering were key here: these are two mathematically complex machine-learning methods developed precisely to discover such meanings and trends in large textual corpora. These methods fall under the umbrella of {\field{\*\fldinst{HYPERLINK "https://jamesgordon1741.github.io/portfolio/project1/2025/10/06/Machine-Learning-and-Digital-Humanities.html"}}{\fldrslt \cf4 \ul \ulc4 \strokec4 unsupervised learning}}, meaning the analysis of data by a computing model operating autonomously, searching for patterns and trends that might take a human researcher weeks or longer to identify.\
Any digital project needs to begin with the assembly of a machine-readable corpus. My corpus consisted of three digitised texts, each between 400 and 600 pages in length, sourced from the Biblioth\'e8que nationale de France, with complete and high-quality scans available online through {\field{\*\fldinst{HYPERLINK "https://gallica.bnf.fr/services/engine/search/sru?operation=searchRetrieve&version=1.2&startRecord=0&maximumRecords=15&page=1&query=%28gallica%20all%20%22lamine%20drame%22%29"}}{\fldrslt \cf4 \ul \ulc4 \strokec4 Gallica}}. All three texts are different recollections of the final campaign against Mahmadou Lamine Dram\'e9, written by prominent French officers and colonial administrators. Preparing the corpus for digital analysis involved, first, acquiring usable text files. Next, the data had to be \'93cleaned\'94 of elements deemed obstructive to analysis, leaving a manipulable mass of tokens. Because my project focuses on semantics, elements such as page numbers had to be ignored by the pipeline, as did punctuation (except when used for sentence segmentation) and extraneous whitespace.\
The French language, which is the language of all the texts, also presents specific challenges for cleaning. These were addressed through normalization, stop-word lists, and lemmatisation. Normalization instructs the pipeline to ignore diacritics, dashes, and other orthographic elements (implemented via 
\f2\fs26 \strokec3 re.sub
\f0\fs24 \strokec3 ). Deciding on stop words was perhaps the most important step, as this choice depends heavily on the type of analysis being performed. French contains many highly frequent short words\'97prepositions, articles, and clitics\'97which, in frequency-based analysis, can introduce \'93noise\'94 and obscure meaningful results. Finally, lemmatisation is essential in languages with many inflected forms, where variants across tenses can appear as distinct words (for example, 
\f1\i \strokec3 avoir \uc0\u8594  avait
\f0\i0 \strokec3  and 
\f1\i \strokec3 \'eatre \uc0\u8594  \'e9tait
\f0\i0 \strokec3 ). Lemmatisation \'93collapses\'94 these variants into a single base form, improving analytical clarity.\
While condensing the number of words and focusing on longer, more distinctive lexical items is one viable approach, fully excluding smaller functional words can become an analytical pitfall in sentence-level analysis. Such words can significantly alter meaning when attached to larger expressions. This consideration motivated my use of a TF-IDF model to automatically identify bigrams and trigrams\'97combined expressions treated as distinct values in my word clouds. For example, expressions such as 
\f1\i \strokec3 \'93nos relations\'94
\f0\i0 \strokec3  emerged as meaningful units (IMAGE HERE IMPORTANT.CLUSTER.0).\
In the early stages of the project, I developed a script that operated on relatively large textual chunks, allowing me to test whether basic frequency-based methods could surface meaningful patterns in the corpus. At this stage, a major focus was the construction of a cleaned and curated list of French stop words, as highly frequent words were obscuring more semantically relevant terms. As the analysis progressed, I reduced chunk size to increase semantic precision and began experimenting with clustering approaches. I ultimately chose a combined TF-IDF and K-Means pipeline, which allowed textual units to be vectorised and grouped based on shared vocabulary.\
My inspiration for the latter phases of the project came primarily from the article {\field{\*\fldinst{HYPERLINK "https://brill.com/view/journals/jdir/3/1/article-p97_4.xml"}}{\fldrslt \cf4 \ul \ulc4 \strokec4 \'9380 Tafsirs, by Text Mining Tafsir: Compilation and Preliminary Explorations of a Curated Corpus of 80 Qur\uc0\u702 anic Commentaries\'94}} by Thomas Jurczyk, Roman Seidel, Adrian Bernhard, Tatjana Scheffler, and Johann Buessow. In this article, the authors performed initial surveys addressing semantic questions in order to demonstrate the viability of various digital models for semantic analysis. While their corpus was larger and more complex than mine, they employed general, largely reproducible methods. Following their approach, I adopted elements of their process, including a sliding-window method, word-cloud construction, and embedding. However, unlike the authors\'97who primarily used BERTopic\'97I relied on TF-IDF for vectorisation (thereby completing the embedding step and making cleaned tokens available for clustering) and for word-cloud construction, and opted for K-Means to perform topic modelling and clustering.\
The first series of clusters proved difficult to interpret, as they lacked inherent labels (PICTURE OF CSV WITHOUT LABELS). To address this, I implemented automatic cluster labelling using TF-IDF feature weights. Even with these improvements, early results remained unsatisfactory: clusters were still dominated by short words and highly frequent verb forms such as 
\f1\i \strokec3 avoir
\f0\i0 \strokec3 . Additionally, stop words applied during early stages were inadvertently reintroduced during clustering in initial versions of the code. I therefore introduced stricter token-length thresholds and lemmatisation to collapse variants into base forms. An example of an early word cloud and a corresponding row from the generated CSV file is included here to illustrate this intermediate stage of analysis.\
Following this, I reoriented the analysis around sentence-level units rather than chunks, reasoning that sentences offered a more meaningful scale for capturing discourse and attribution. I also implemented a soft-filtering ruleset to group references to Mahmadou (e.g., 
\f1\i \strokec3 Mahmadou
\f0\i0 \strokec3 , 
\f1\i \strokec3 Lamin\'e9
\f0\i0 \strokec3 , and the shorthand 
\f1\i \strokec3 \'93le marabout\'94
\f0\i0 \strokec3 ) in order to focus more closely on my primary actors. This shift required further refinement of cleaning and lemmatisation procedures, after which I explored the use of bigrams and trigrams through TF-IDF vectorisation to capture recurring multi-word expressions. This also enabled the construction of contrastive word clouds, which highlight terms statistically distinctive to particular clusters rather than merely frequent across the corpus. Both standard frequency-based and contrastive word clouds are presented to demonstrate the difference in interpretive value.\
At this stage, the codebase had a tripartite structure: the first script normalised the data and generated the CSV; the second imported K-Means and assembled the clusters; and the third\'97added later\'97used TF-IDF to automatically label clusters. At times, I also experimented with a fourth script examining distributions.\
Before concluding, it is worth noting what this project does 
\f1\i \strokec3 not
\f0\i0 \strokec3  attempt. While clustering was central to the analysis, I did not pursue alternative visualisations such as two-dimensional embedding plots (e.g. PCA or t-SNE), instead prioritising textual interpretability over spatial representation. A matrix-based distribution would also be useful for more fully understanding the data, and I hope to develop such models in future work to refine and extend the present results. Nonetheless, the word clouds alone already provoke meaningful reflections on the texts and their semantics.\
I was particularly interested in the word clouds for clusters 0 and 2 (IMAGE HERE: CLUSTERS 0 AND 2, DEALING WITH RELATIONS 
\f1\i \strokec3 AVEC
\f0\i0 \strokec3  AHMADOU AND 
\f1\i \strokec3 CONTRE
\f0\i0 \strokec3  MAHMADOU). These reveal that notions of total war without negotiation are associated with cluster 0, dominated by references to Mahmadou and his various names. By contrast, conciliatory and diplomatic\'97though still militarised\'97relations are associated with Mahmadou\'92s regional rival and France\'92s temporary strategic partner, the \'93Sultan\'94 Ahmadou, who dominates cluster 2. These patterns offer two coherent interpretive avenues relevant to my thesis concerning the distinct semantic spaces occupied by Mahmadou and Ahmadou in French colonial writings of the period.\
This project demonstrated that producing code for semantic analysis can be an arduous process. Filtering out data that was removed and later reintroduced at different stages of the pipeline proved to be a recurring challenge. Nevertheless, the project also showed that cluster-based analysis can be particularly fruitful, even for smaller-scale corpora, and can open up new avenues for historical and textual exploration.\
}
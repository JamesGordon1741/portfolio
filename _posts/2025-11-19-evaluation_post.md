reflecting on approaches to evaluating HTR. Are these numerical measures sufficient?

ead the following blog post about evaluation: https://towardsdatascience.com/evaluating-ocr-output-quality-with-character-error-rate-cer-and-word-error-rate-wer-853175297510/

Then write a blog on your portfolio website, reflecting on approaches to evaluating HTR. Are these numerical measures sufficient? When might the not be very helpful? Maximum 500 words.


Several numerical values are used to evaluate HTR. These incldue WAR, CAR, CER, and WER, which mathematically represent the difference between a ground truth, or an accurate, manual transcription of document, and the automatic transcription attempted by a given model. While HTR values are sufficient, broadly, for providing feedback to develop a model for general use of texts in a given language, the values might cover up certain factors which can be useful when analyzing texts. It is common to have different expectations for different kind of texts when the relative effeciency of a model based on its CER value. For some standardized texts, such as those written on a typewriter or printed from computer text in a common, romanized language, nothing less than a CER of 1% can be considered a good outcome. Conversely, a benchmark of 20% can be good for an idiosyncratically handwritten and obscure content, whether this be language or spelling. Some scribal traditions might be less known to models, due to language or orthographic practices, and resistant to models despite their uniformity to eachother. In this case, the HTR values would not tell you alot about why a model is struggling with given letters for a larger, and you would be unable to know, in theory,  unless you painstakingly look through both the ground truth and the model-transcribed text. This would, in turn, prove complicated for larger jobs, of texts hundreds of pages in length.
